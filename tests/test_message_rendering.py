"""
Tests for Message Rendering System
===================================
"""

import pytest
import torch
from transformers import AutoTokenizer

from autotrain.rendering import (
    AlpacaRenderer,
    ChatFormat,
    ChatMLRenderer,
    Conversation,
    LlamaRenderer,
    Message,
    MessageRenderer,
    MistralRenderer,
    RenderConfig,
    TokenWeight,
    VicunaRenderer,
    ZephyrRenderer,
    apply_token_weights,
    build_generation_prompt,
    build_supervised_example,
    convert_dataset_to_conversations,
    create_chat_template,
    detect_chat_format,
    get_renderer,
    get_stop_sequences,
    parse_response,
)


@pytest.fixture
def tokenizer():
    """Create tokenizer for testing."""
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    return tokenizer


@pytest.fixture
def sample_conversation():
    """Create sample conversation."""
    return Conversation(
        messages=[
            Message(role="system", content="You are a helpful assistant."),
            Message(role="user", content="What is 2+2?"),
            Message(role="assistant", content="2+2 equals 4."),
        ]
    )


def test_message_creation():
    """Test Message creation."""
    msg = Message(role="user", content="Hello", weight=0.5)
    assert msg.role == "user"
    assert msg.content == "Hello"
    assert msg.weight == 0.5


def test_conversation_creation():
    """Test Conversation creation and methods."""
    conv = Conversation(messages=[])
    assert len(conv.messages) == 0

    conv.add_message("user", "Hello")
    assert len(conv.messages) == 1
    assert conv.messages[0].role == "user"

    # Test to_dict
    data = conv.to_dict()
    assert "messages" in data
    assert len(data["messages"]) == 1

    # Test from_dict
    conv2 = Conversation.from_dict(data)
    assert len(conv2.messages) == 1
    assert conv2.messages[0].content == "Hello"


def test_token_weight():
    """Test TokenWeight functionality."""
    weight = TokenWeight(start_idx=10, end_idx=20, weight=0.5)

    tensor = torch.ones(30)
    weighted = weight.apply_to_tensor(tensor)

    assert weighted[0] == 1.0  # Before range
    assert weighted[15] == 0.5  # In range
    assert weighted[25] == 1.0  # After range


def test_render_config():
    """Test RenderConfig."""
    config = RenderConfig(
        format=ChatFormat.CHATML,
        max_length=512,
        only_assistant=True,
    )

    assert config.format == ChatFormat.CHATML
    assert config.max_length == 512
    assert config.only_assistant == True


def test_chatml_renderer(tokenizer, sample_conversation):
    """Test ChatML rendering."""
    config = RenderConfig(format=ChatFormat.CHATML)
    renderer = ChatMLRenderer(tokenizer, config)

    # Test conversation rendering
    rendered = renderer.render_conversation(sample_conversation)
    assert "<|im_start|>" in rendered
    assert "<|im_end|>" in rendered
    assert "system" in rendered
    assert "You are a helpful assistant." in rendered

    # Test stop sequences
    stops = renderer.get_stop_sequences()
    assert "<|im_end|>" in stops

    # Test generation prompt
    prompt = renderer.build_generation_prompt(sample_conversation)
    assert prompt.endswith("<|im_start|>assistant\n")

    # Test response parsing
    response = "<|im_start|>assistant\nThis is a response<|im_end|>"
    parsed = renderer.parse_response(response)
    assert "This is a response" in parsed
    assert "<|im_start|>" not in parsed


def test_alpaca_renderer(tokenizer, sample_conversation):
    """Test Alpaca rendering."""
    config = RenderConfig(format=ChatFormat.ALPACA)
    renderer = AlpacaRenderer(tokenizer, config)

    rendered = renderer.render_conversation(sample_conversation)
    assert "### Instruction:" in rendered
    assert "### Response:" in rendered

    stops = renderer.get_stop_sequences()
    assert "### Instruction:" in stops


def test_vicuna_renderer(tokenizer, sample_conversation):
    """Test Vicuna rendering."""
    renderer = VicunaRenderer(tokenizer, RenderConfig())

    rendered = renderer.render_conversation(sample_conversation)
    assert "USER:" in rendered
    assert "ASSISTANT:" in rendered

    stops = renderer.get_stop_sequences()
    assert "USER:" in stops


def test_zephyr_renderer(tokenizer, sample_conversation):
    """Test Zephyr rendering."""
    renderer = ZephyrRenderer(tokenizer, RenderConfig())

    rendered = renderer.render_conversation(sample_conversation)
    assert "<|system|>" in rendered
    assert "<|user|>" in rendered
    assert "<|assistant|>" in rendered
    assert "</s>" in rendered


def test_llama_renderer(tokenizer):
    """Test Llama rendering."""
    renderer = LlamaRenderer(tokenizer, RenderConfig())

    conv = Conversation(
        messages=[
            Message(role="system", content="System prompt"),
            Message(role="user", content="Question"),
            Message(role="assistant", content="Answer"),
        ]
    )

    rendered = renderer.render_conversation(conv)
    assert "[INST]" in rendered
    assert "[/INST]" in rendered
    assert "<<SYS>>" in rendered
    assert "<</SYS>>" in rendered


def test_mistral_renderer(tokenizer):
    """Test Mistral rendering."""
    renderer = MistralRenderer(tokenizer, RenderConfig())

    conv = Conversation(
        messages=[
            Message(role="user", content="Question"),
            Message(role="assistant", content="Answer"),
        ]
    )

    rendered = renderer.render_conversation(conv)
    assert "[INST]" in rendered
    assert "[/INST]" in rendered


def test_get_renderer(tokenizer):
    """Test renderer factory."""
    renderer = get_renderer(ChatFormat.CHATML, tokenizer)
    assert isinstance(renderer, ChatMLRenderer)

    renderer = get_renderer("alpaca", tokenizer)
    assert isinstance(renderer, AlpacaRenderer)

    # Test with custom config
    config = RenderConfig(format=ChatFormat.VICUNA, max_length=256)
    renderer = get_renderer(ChatFormat.VICUNA, tokenizer, config)
    assert renderer.config.max_length == 256


def test_tokenize_conversation(tokenizer, sample_conversation):
    """Test conversation tokenization."""
    config = RenderConfig(
        format=ChatFormat.CHATML,
        max_length=128,
        only_assistant=True,
    )
    renderer = ChatMLRenderer(tokenizer, config)

    encoded = renderer.tokenize_conversation(sample_conversation)

    assert "input_ids" in encoded
    assert "attention_mask" in encoded
    assert "labels" in encoded
    assert "token_weights" in encoded

    # Check shapes
    assert encoded["input_ids"].shape[-1] == 128  # max_length
    assert encoded["labels"].shape == encoded["input_ids"].shape


def test_token_weights_computation(tokenizer):
    """Test token weight computation for training."""
    config = RenderConfig(
        format=ChatFormat.CHATML,
        only_assistant=True,
        mask_user=True,
        mask_system=True,
    )
    renderer = ChatMLRenderer(tokenizer, config)

    conv = Conversation(
        messages=[
            Message(role="system", content="System"),
            Message(role="user", content="User"),
            Message(role="assistant", content="Assistant"),
        ]
    )

    encoded = renderer.tokenize_conversation(conv)
    weights = encoded["token_weights"]

    # Weights should be 0 for system/user, 1 for assistant
    assert weights.min() == 0  # Some tokens masked
    assert weights.max() == 1  # Some tokens not masked


def test_build_generation_prompt():
    """Test generation prompt building."""
    messages = [
        {"role": "user", "content": "Hello"},
    ]

    prompt = build_generation_prompt(messages, ChatFormat.CHATML)
    assert "<|im_start|>user" in prompt
    assert "Hello" in prompt
    assert prompt.endswith("<|im_start|>assistant\n")


def test_parse_response():
    """Test response parsing."""
    response = "<|im_start|>assistant\nHello there!<|im_end|>"
    parsed = parse_response(response, ChatFormat.CHATML)
    assert parsed == "Hello there!"

    response = "### Response:\nHello there!\n### Instruction:"
    parsed = parse_response(response, ChatFormat.ALPACA)
    assert "Hello there!" in parsed
    assert "###" not in parsed


def test_get_stop_sequences():
    """Test stop sequence retrieval."""
    stops = get_stop_sequences(ChatFormat.CHATML)
    assert "<|im_end|>" in stops

    stops = get_stop_sequences(ChatFormat.ALPACA)
    assert "### Instruction:" in stops


def test_build_supervised_example(tokenizer):
    """Test supervised example building."""
    messages = [
        {"role": "user", "content": "Question"},
        {"role": "assistant", "content": "Answer"},
    ]

    example = build_supervised_example(
        messages,
        tokenizer,
        ChatFormat.CHATML,
        max_length=64,
        only_assistant=True,
    )

    assert "input_ids" in example
    assert "labels" in example
    assert "token_weights" in example
    assert example["input_ids"].shape[-1] == 64


def test_apply_token_weights():
    """Test token weight application."""
    input_ids = torch.ones(10)

    # Test with weight list
    weights = [
        TokenWeight(0, 3, 0.5),
        TokenWeight(7, 10, 0.0),
    ]

    weighted = apply_token_weights(input_ids, weights)
    assert weighted[1] == 0.5
    assert weighted[5] == 1.0
    assert weighted[8] == 0.0

    # Test with tensor weights
    weight_tensor = torch.tensor([0.5] * 10)
    weighted = apply_token_weights(input_ids, weight_tensor)
    assert torch.all(weighted == 0.5)


def test_detect_chat_format():
    """Test chat format detection."""
    fmt = detect_chat_format("qwen-chat")
    assert fmt == ChatFormat.CHATML

    fmt = detect_chat_format("alpaca-7b")
    assert fmt == ChatFormat.ALPACA

    fmt = detect_chat_format("llama-2-chat")
    assert fmt == ChatFormat.LLAMA

    fmt = detect_chat_format("unknown-model")
    assert fmt == ChatFormat.CHATML  # Default


def test_convert_dataset_to_conversations():
    """Test dataset conversion."""
    # Test with list of dicts
    dataset = [
        {"text": "This is text 1"},
        {"text": "This is text 2"},
    ]

    conversations = convert_dataset_to_conversations(dataset, text_column="text")
    assert len(conversations) == 2
    assert conversations[0].messages[1].content == "This is text 1"

    # Test with conversation format
    dataset = [
        {
            "messages": [
                {"role": "user", "content": "Q1"},
                {"role": "assistant", "content": "A1"},
            ]
        },
    ]

    conversations = convert_dataset_to_conversations(dataset)
    assert len(conversations) == 1
    assert len(conversations[0].messages) == 2


def test_create_chat_template():
    """Test Jinja2 template creation."""
    template = create_chat_template(ChatFormat.CHATML)
    assert "<|im_start|>" in template
    assert "message['role']" in template

    template = create_chat_template(ChatFormat.ALPACA)
    assert "### Instruction:" in template

    # Test custom template
    template = create_chat_template(
        ChatFormat.CUSTOM,
        user_template="USER: {content}",
        assistant_template="BOT: {content}",
    )
    assert "USER:" in template
    assert "BOT:" in template


def test_custom_renderer(tokenizer):
    """Test custom renderer with templates."""
    config = RenderConfig(
        format=ChatFormat.CUSTOM,
        system_template="System: {content}",
        user_template="Human: {content}",
        assistant_template="AI: {content}",
    )

    from autotrain.rendering.message_renderer import CustomRenderer

    renderer = CustomRenderer(tokenizer, config)

    conv = Conversation(
        messages=[
            Message(role="system", content="Be helpful"),
            Message(role="user", content="Hello"),
            Message(role="assistant", content="Hi there"),
        ]
    )

    rendered = renderer.render_conversation(conv)
    assert "System: Be helpful" in rendered
    assert "Human: Hello" in rendered
    assert "AI: Hi there" in rendered


class TestToolRoleConversion:
    """Tests for tool role conversion in TokenizerNativeRenderer.

    Gemma and other models require strict user/assistant alternation and don't
    support 'tool' role. These tests verify that tool messages are correctly
    converted to user messages when the tokenizer doesn't support them.
    """

    def test_check_tool_role_support_gemma(self):
        """Test that Gemma is correctly detected as NOT supporting tool role."""
        from autotrain.rendering.message_renderer import TokenizerNativeRenderer

        # Use Gemma tokenizer which doesn't support tool role
        try:
            tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")
        except Exception:
            pytest.skip("Gemma tokenizer not available")

        config = RenderConfig(format=ChatFormat.NATIVE)
        renderer = TokenizerNativeRenderer(tokenizer, config)

        # Gemma should NOT support tool role
        assert renderer._check_tool_role_support() == False

    def test_check_tool_role_support_caching(self, tokenizer):
        """Test that tool role support check is cached."""
        from autotrain.rendering.message_renderer import TokenizerNativeRenderer

        config = RenderConfig(format=ChatFormat.NATIVE)
        renderer = TokenizerNativeRenderer(tokenizer, config)

        # First call should set the cache
        result1 = renderer._check_tool_role_support()
        assert renderer._supports_tool_role is not None

        # Second call should use cache (same result)
        result2 = renderer._check_tool_role_support()
        assert result1 == result2

    def test_preprocess_tool_role_to_user(self, tokenizer):
        """Test that tool role is converted to user role."""
        from autotrain.rendering.message_renderer import TokenizerNativeRenderer

        config = RenderConfig(format=ChatFormat.NATIVE)
        renderer = TokenizerNativeRenderer(tokenizer, config)

        messages = [
            {"role": "user", "content": "What is 2+2?"},
            {"role": "assistant", "content": "Let me calculate that."},
            {"role": "tool", "content": "Result: 4"},
            {"role": "assistant", "content": "The answer is 4."},
        ]

        processed = renderer._preprocess_messages_for_alternation(messages)

        # Should have 4 messages (tool converted to user)
        assert len(processed) == 4
        # Tool message should now be user with [Tool Result] prefix
        assert processed[2]["role"] == "user"
        assert processed[2]["content"] == "[Tool Result] Result: 4"

    def test_preprocess_merges_consecutive_same_role(self, tokenizer):
        """Test that consecutive same-role messages are merged."""
        from autotrain.rendering.message_renderer import TokenizerNativeRenderer

        config = RenderConfig(format=ChatFormat.NATIVE)
        renderer = TokenizerNativeRenderer(tokenizer, config)

        # Two consecutive user messages (e.g., user + tool converted to user)
        messages = [
            {"role": "user", "content": "First message"},
            {"role": "user", "content": "Second message"},
            {"role": "assistant", "content": "Response"},
        ]

        processed = renderer._preprocess_messages_for_alternation(messages)

        # Should merge the two user messages
        assert len(processed) == 2
        assert "First message" in processed[0]["content"]
        assert "Second message" in processed[0]["content"]

    def test_preprocess_complex_tool_sequence(self, tokenizer):
        """Test complex sequence with multiple tool calls."""
        from autotrain.rendering.message_renderer import TokenizerNativeRenderer

        config = RenderConfig(format=ChatFormat.NATIVE)
        renderer = TokenizerNativeRenderer(tokenizer, config)

        messages = [
            {"role": "user", "content": "Check the weather"},
            {"role": "assistant", "content": "Checking..."},
            {"role": "tool", "content": "Weather: sunny"},
            {"role": "tool", "content": "Temp: 20C"},  # Multiple tool results
            {"role": "assistant", "content": "It's sunny and 20C"},
        ]

        processed = renderer._preprocess_messages_for_alternation(messages)

        # user, assistant, user (merged tools), assistant = 4 messages
        assert len(processed) == 4
        # Both tool results should be merged into one user message
        assert "[Tool Result] Weather: sunny" in processed[2]["content"]
        assert "[Tool Result] Temp: 20C" in processed[2]["content"]

    def test_preprocess_preserves_system_role(self, tokenizer):
        """Test that system role is preserved."""
        from autotrain.rendering.message_renderer import TokenizerNativeRenderer

        config = RenderConfig(format=ChatFormat.NATIVE)
        renderer = TokenizerNativeRenderer(tokenizer, config)

        messages = [
            {"role": "system", "content": "You are helpful"},
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi!"},
        ]

        processed = renderer._preprocess_messages_for_alternation(messages)

        assert len(processed) == 3
        assert processed[0]["role"] == "system"
        assert processed[0]["content"] == "You are helpful"

    def test_preprocess_handles_empty_messages(self, tokenizer):
        """Test handling of empty message list."""
        from autotrain.rendering.message_renderer import TokenizerNativeRenderer

        config = RenderConfig(format=ChatFormat.NATIVE)
        renderer = TokenizerNativeRenderer(tokenizer, config)

        processed = renderer._preprocess_messages_for_alternation([])
        assert processed == []

    def test_preprocess_handles_none_content(self, tokenizer):
        """Test handling of None content in messages."""
        from autotrain.rendering.message_renderer import TokenizerNativeRenderer

        config = RenderConfig(format=ChatFormat.NATIVE)
        renderer = TokenizerNativeRenderer(tokenizer, config)

        messages = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": None},  # Some APIs return None for empty
            {"role": "tool", "content": "Result"},
            {"role": "assistant", "content": "Done"},
        ]

        processed = renderer._preprocess_messages_for_alternation(messages)

        # Should handle None gracefully
        assert len(processed) == 4
        assert processed[1]["content"] == ""  # None converted to empty string

    @pytest.mark.skipif(
        not pytest.importorskip("transformers", reason="transformers not available"),
        reason="requires transformers"
    )
    def test_render_with_tool_role_gemma_style(self):
        """Test rendering with tool role using a tokenizer that requires alternation.

        This simulates what happens when training data with tool messages is used
        with Gemma or similar models that require strict alternation.
        """
        from autotrain.rendering.message_renderer import TokenizerNativeRenderer

        # Use GPT2 as a stand-in (it doesn't have a chat template, but that's ok)
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
        tokenizer.pad_token = tokenizer.eos_token

        config = RenderConfig(format=ChatFormat.NATIVE)
        renderer = TokenizerNativeRenderer(tokenizer, config)

        conv = Conversation(
            messages=[
                Message(role="user", content="Calculate 2+2"),
                Message(role="assistant", content="Let me use a calculator"),
                Message(role="tool", content="4"),
                Message(role="assistant", content="The result is 4"),
            ]
        )

        # This should not raise an error
        rendered = renderer.render_conversation(conv)

        # The tool message should be converted to user format
        assert "[Tool Result]" in rendered or "4" in rendered


class TestSafeApplyChatTemplate:
    """Tests for the safe_apply_chat_template centralized utility."""

    def test_safe_apply_preserves_native_tool_support(self):
        """Test that models with native tool support keep tool messages as-is."""
        from autotrain.rendering.utils import safe_apply_chat_template, check_tool_role_support

        # Qwen supports tool role natively
        try:
            tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
        except Exception:
            pytest.skip("Qwen tokenizer not available")

        # Verify Qwen supports tool
        assert check_tool_role_support(tokenizer) == True

        messages = [
            {"role": "user", "content": "Calculate 2+2"},
            {"role": "assistant", "content": "Let me use a tool"},
            {"role": "tool", "content": "4"},
            {"role": "assistant", "content": "The answer is 4"},
        ]

        # Should NOT add [Tool Result] prefix for models that support tool natively
        result = safe_apply_chat_template(tokenizer, messages, tokenize=False)
        assert "[Tool Result]" not in result

    def test_safe_apply_converts_for_gemma(self):
        """Test that Gemma gets tool messages converted."""
        from autotrain.rendering.utils import safe_apply_chat_template, check_tool_role_support

        try:
            tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")
        except Exception:
            pytest.skip("Gemma tokenizer not available")

        # Verify Gemma doesn't support tool
        assert check_tool_role_support(tokenizer) == False

        messages = [
            {"role": "user", "content": "Calculate 2+2"},
            {"role": "assistant", "content": "Let me calculate"},
            {"role": "tool", "content": "Result: 4"},
            {"role": "assistant", "content": "The answer is 4"},
        ]

        # Should convert tool to user with [Tool Result] prefix
        result = safe_apply_chat_template(tokenizer, messages, tokenize=False)
        assert "[Tool Result]" in result
        assert "Result: 4" in result

    def test_safe_apply_no_tool_messages_unchanged(self):
        """Test that messages without tool role are unchanged."""
        from autotrain.rendering.utils import safe_apply_chat_template

        # Use a tokenizer with a chat template
        try:
            tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-1.5B-Instruct")
        except Exception:
            pytest.skip("Qwen tokenizer not available")

        messages = [
            {"role": "user", "content": "Hello"},
            {"role": "assistant", "content": "Hi there!"},
        ]

        # Should work normally without any conversion
        result = safe_apply_chat_template(tokenizer, messages, tokenize=False)
        assert "Hello" in result
        assert "Hi there" in result

    def test_cache_is_used(self):
        """Test that tool role support detection is cached."""
        from autotrain.rendering.utils import _tool_role_support_cache, check_tool_role_support

        try:
            tokenizer = AutoTokenizer.from_pretrained("google/gemma-3-1b-it")
        except Exception:
            pytest.skip("Gemma tokenizer not available")

        # First call populates cache
        result1 = check_tool_role_support(tokenizer)

        # Check cache has the entry
        tokenizer_id = tokenizer.name_or_path
        assert tokenizer_id in _tool_role_support_cache

        # Second call uses cache
        result2 = check_tool_role_support(tokenizer)
        assert result1 == result2
